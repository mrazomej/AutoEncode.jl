<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Encoders &amp; Decoders Â· AutoEncode</title><meta name="title" content="Encoders &amp; Decoders Â· AutoEncode"/><meta property="og:title" content="Encoders &amp; Decoders Â· AutoEncode"/><meta property="twitter:title" content="Encoders &amp; Decoders Â· AutoEncode"/><meta name="description" content="Documentation for AutoEncode."/><meta property="og:description" content="Documentation for AutoEncode."/><meta property="twitter:description" content="Documentation for AutoEncode."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.svg" alt="AutoEncode logo"/><img class="docs-dark-only" src="../assets/logo-dark.svg" alt="AutoEncode logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoEncode</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Encoders &amp; Decoders</a><ul class="internal"><li><a class="tocitem" href="#Encoders"><span>Encoders</span></a></li><li><a class="tocitem" href="#Decoders"><span>Decoders</span></a></li><li><a class="tocitem" href="#Default-initializations"><span>Default initializations</span></a></li><li><a class="tocitem" href="#Probabilistic-functions"><span>Probabilistic functions</span></a></li><li><a class="tocitem" href="#Defining-custom-encoder-and-decoder-types"><span>Defining custom encoder and decoder types</span></a></li></ul></li><li><a class="tocitem" href="../ae/">Deterministic Autoencoders</a></li><li><a class="tocitem" href="../vae/">VAE / Î²-VAE</a></li><li><a class="tocitem" href="../mmdvae/">MMD-VAE (InfoVAE)</a></li><li><a class="tocitem" href="../infomaxvae/">InfoMax-VAE</a></li><li><a class="tocitem" href="../hvae/">HVAE</a></li><li><a class="tocitem" href="../rhvae/">RHVAE</a></li><li><a class="tocitem" href="../diffgeo/">Differential Geometry</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Encoders &amp; Decoders</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Encoders &amp; Decoders</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Encoders-and-Decoders"><a class="docs-heading-anchor" href="#Encoders-and-Decoders">Encoders &amp; Decoders</a><a id="Encoders-and-Decoders-1"></a><a class="docs-heading-anchor-permalink" href="#Encoders-and-Decoders" title="Permalink"></a></h1><p><code>AutoEncode.jl</code> provides a set of predefined encoders and decoders that can be used to define custom (variational) autoencoder architectures.</p><h2 id="Encoders"><a class="docs-heading-anchor" href="#Encoders">Encoders</a><a id="Encoders-1"></a><a class="docs-heading-anchor-permalink" href="#Encoders" title="Permalink"></a></h2><p>The tree structure of the encoder types looks like this (ðŸ§± represents concrete types):</p><ul><li><code>AbstractEncoder</code><ul><li><code>AbstractDeterministicEncoder</code><ul><li><a href="#Encoder"><code>Encoder</code> ðŸ§±</a></li></ul></li><li><code>AbstractVariationalEncoder</code><ul><li><code>AbstractGaussianEncoder</code><ul><li><code>AbstractGaussianLinearEncoder</code><ul><li><a href="#JointEncoder"><code>JointEncoder</code> ðŸ§±</a></li></ul></li><li><code>AbstractGaussianLogEncoder</code><ul><li><a href="#JointLogEncoder"><code>JointLogEncoder</code> ðŸ§±</a></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="Encoder"><a class="docs-heading-anchor" href="#Encoder"><code>Encoder</code></a><a id="Encoder-1"></a><a class="docs-heading-anchor-permalink" href="#Encoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.Encoder" href="#AutoEncode.Encoder"><code>AutoEncode.Encoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><p><code>struct Encoder</code></p><p>Default encoder function for deterministic autoencoders. The <code>encoder</code> network is used to map the input data directly into the latent space representation.</p><p><strong>Fields</strong></p><ul><li><code>encoder::Union{Flux.Chain,Flux.Dense}</code>: The primary neural network used to process input data and map it into a latent space representation.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">enc = Encoder(Flux.Chain(Dense(784, 400, relu), Dense(400, 20)))</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.Encoder-Tuple{AbstractArray}" href="#AutoEncode.Encoder-Tuple{AbstractArray}"><code>AutoEncode.Encoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(encoder::Encoder)(x)</code></pre><p>Forward propagate the input <code>x</code> through the <code>Encoder</code> to obtain the encoded representation in the latent space.</p><p><strong>Arguments</strong></p><ul><li><code>x::Array</code>: Input data to be encoded.</li></ul><p><strong>Returns</strong></p><ul><li><code>z</code>: Encoded representation of the input data in the latent space.</li></ul><p><strong>Description</strong></p><p>This method allows for a direct call on an instance of <code>Encoder</code> with the input data <code>x</code>. It runs the input through the encoder network and outputs the encoded representation in the latent space.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">enc = Encoder(...)
z = enc(some_input)</code></pre><p><strong>Note</strong></p><p>Ensure that the input x matches the expected dimensionality of the encoder&#39;s input layer.</p></div></section></article><h3 id="JointEncoder"><a class="docs-heading-anchor" href="#JointEncoder"><code>JointEncoder</code></a><a id="JointEncoder-1"></a><a class="docs-heading-anchor-permalink" href="#JointEncoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointEncoder" href="#AutoEncode.JointEncoder"><code>AutoEncode.JointEncoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><p><code>struct JointEncoder &lt;: AbstractGaussianLinearEncoder</code></p><p>Encoder function for variational autoencoders where the same <code>encoder</code> network is used to map to the latent space mean <code>Âµ</code> and standard deviation <code>Ïƒ</code>.</p><p><strong>Fields</strong></p><ul><li><code>encoder::Flux.Chain</code>: The primary neural network used to process input data and map it into a latent space representation.</li><li><code>Âµ::Flux.Dense</code>: A dense layer mapping from the output of the <code>encoder</code> to the mean of the latent space.</li><li><code>Ïƒ::Flux.Dense</code>: A dense layer mapping from the output of the <code>encoder</code> to the standard deviation of the latent space.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">enc = JointEncoder(
    Flux.Chain(Dense(784, 400, relu)), Flux.Dense(400, 20), Flux.Dense(400, 20)
)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointEncoder-Tuple{AbstractArray}" href="#AutoEncode.JointEncoder-Tuple{AbstractArray}"><code>AutoEncode.JointEncoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (encoder::JointEncoder)(x::AbstractArray)</code></pre><p>Forward propagate the input <code>x</code> through the <code>JointEncoder</code> to obtain the mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>) of the latent space.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: Input data to be encoded.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(Âµ=Âµ, Ïƒ=Ïƒ,)</code> where:<ul><li><code>Âµ</code>: Mean of the latent space after passing the input through the encoder and subsequently through the <code>Âµ</code> layer.</li><li><code>Ïƒ</code>: Standard deviation of the latent space after passing the input through the encoder and subsequently through the <code>Ïƒ</code> layer.</li></ul></li></ul><p><strong>Description</strong></p><p>This method allows for a direct call on an instance of <code>JointEncoder</code> with the input data <code>x</code>. It first runs the input through the encoder network, then maps the output of the last encoder layer to both the mean and standard deviation of the latent space.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">je = JointEncoder(...)
Âµ, Ïƒ = je(some_input)</code></pre><p><strong>Note</strong></p><p>Ensure that the input x matches the expected dimensionality of the encoder&#39;s input layer.</p></div></section></article><h3 id="JointLogEncoder"><a class="docs-heading-anchor" href="#JointLogEncoder"><code>JointLogEncoder</code></a><a id="JointLogEncoder-1"></a><a class="docs-heading-anchor-permalink" href="#JointLogEncoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogEncoder" href="#AutoEncode.JointLogEncoder"><code>AutoEncode.JointLogEncoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><p><code>struct JointLogEncoder &lt;: AbstractGaussianLogEncoder</code></p><p>Default encoder function for variational autoencoders where the same <code>encoder</code> network is used to map to the latent space mean <code>Âµ</code> and log standard deviation <code>logÏƒ</code>.</p><p><strong>Fields</strong></p><ul><li><code>encoder::Flux.Chain</code>: The primary neural network used to process input data and map it into a latent space representation.</li><li><code>Âµ::Union{Flux.Dense,Flux.Chain}</code>: A dense layer or a chain of layers mapping from the output of the <code>encoder</code> to the mean of the latent space.</li><li><code>logÏƒ::Union{Flux.Dense,Flux.Chain}</code>: A dense layer or a chain of layers mapping from the output of the <code>encoder</code> to the log standard deviation of the latent space.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">enc = JointLogEncoder(
        Flux.Chain(Dense(784, 400, relu)), Flux.Dense(400, 20), Flux.Dense(400, 20)
)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogEncoder-Tuple{AbstractArray}" href="#AutoEncode.JointLogEncoder-Tuple{AbstractArray}"><code>AutoEncode.JointLogEncoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (encoder::JointLogEncoder)(x)</code></pre><p>This method forward propagates the input <code>x</code> through the <code>JointLogEncoder</code> to compute the mean (<code>mu</code>) and log standard deviation (<code>logÏƒ</code>) of the latent space.</p><p><strong>Arguments</strong></p><ul><li><code>x::Array{Float32}</code>: The input data to be encoded.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(Âµ=Âµ, logÏƒ=logÏƒ,)</code> where:<ul><li><code>Âµ</code>: The mean of the latent space. This is computed by passing the input through the encoder and subsequently through the <code>Âµ</code> layer.  </li><li><code>logÏƒ</code>: The log standard deviation of the latent space. This is computed by passing the input through the encoder and subsequently through the <code>logÏƒ</code> layer.</li></ul></li></ul><p><strong>Description</strong></p><p>This method allows for a direct call on an instance of <code>JointLogEncoder</code> with the input data <code>x</code>. It first processes the input through the encoder network, then maps the output of the last encoder layer to both the mean and log standard deviation of the latent space.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">je = JointLogEncoder(...)
mu, logÏƒ = je(some_input)</code></pre><p><strong>Note</strong></p><p>Ensure that the input x matches the expected dimensionality of the encoder&#39;s input layer.</p></div></section></article><h2 id="Decoders"><a class="docs-heading-anchor" href="#Decoders">Decoders</a><a id="Decoders-1"></a><a class="docs-heading-anchor-permalink" href="#Decoders" title="Permalink"></a></h2><p>The tree structure of the decoder types looks like this (ðŸ§± represents concrete types):</p><ul><li><code>AbstractDecoder</code><ul><li><code>AbstractDeterministicDecoder</code><ul><li><a href="#Decoder"><code>Decoder</code> ðŸ§±</a></li></ul></li><li><code>AbstractVariationalDecoder</code><ul><li><a href="#BernoulliDecoder"><code>BernoulliDecoder</code> ðŸ§±</a></li><li><a href="#CategoricalDecoder"><code>CategoricalDecoder</code> ðŸ§±</a></li><li><code>AbstractGaussianDecoder</code><ul><li><code>AbstractGaussianLinearDecoder</code><ul><li><a href="#JointDecoder"><code>JointDecoder</code> ðŸ§±</a></li><li><a href="#SplitDecoder"><code>SplitDecoder</code> ðŸ§±</a></li></ul></li><li><code>AbstractGaussianLogDecoder</code><ul><li><a href="#JointLogDecoder"><code>JointLogDecoder</code> ðŸ§±</a></li><li><a href="#SplitLogDecoder"><code>SplitLogDecoder</code> ðŸ§±</a></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id="Decoder"><a class="docs-heading-anchor" href="#Decoder"><code>Decoder</code></a><a id="Decoder-1"></a><a class="docs-heading-anchor-permalink" href="#Decoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.Decoder" href="#AutoEncode.Decoder"><code>AutoEncode.Decoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><p><code>struct Decoder</code></p><p>Default decoder function for deterministic autoencoders. The <code>decoder</code> network is used to map the latent space representation directly back to the original data space.</p><p><strong>Fields</strong></p><ul><li><code>decoder::Flux.Chain</code>: The primary neural network used to process the latent space representation and map it back to the data space.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">dec = Decoder(Flux.Chain(Dense(20, 400, relu), Dense(400, 784)))</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.Decoder-Tuple{AbstractArray}" href="#AutoEncode.Decoder-Tuple{AbstractArray}"><code>AutoEncode.Decoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(decoder::Decoder)(z::AbstractArray)</code></pre><p>Forward propagate the encoded representation <code>z</code> through the <code>Decoder</code> to obtain the reconstructed input data.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: Encoded representation in the latent space.</li></ul><p><strong>Returns</strong></p><ul><li><code>x_reconstructed</code>: Reconstructed version of the original input data after decoding from the latent space.</li></ul><p><strong>Description</strong></p><p>This method allows for a direct call on an instance of <code>Decoder</code> with the encoded data <code>z</code>. It runs the encoded representation through the decoder network and outputs the reconstructed version of the original input data.</p><p><strong>Example</strong></p><p><code>julia dec = Decoder(...) x_reconstructed = dec(encoded_representation)</code>`</p><p><strong>Note</strong></p><p>Ensure that the input z matches the expected dimensionality of the decoder&#39;s input layer.</p></div></section></article><h3 id="BernoulliDecoder"><a class="docs-heading-anchor" href="#BernoulliDecoder"><code>BernoulliDecoder</code></a><a id="BernoulliDecoder-1"></a><a class="docs-heading-anchor-permalink" href="#BernoulliDecoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.BernoulliDecoder" href="#AutoEncode.BernoulliDecoder"><code>AutoEncode.BernoulliDecoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">    BernoulliDecoder &lt;: AbstractVariationalDecoder</code></pre><p>A decoder structure for variational autoencoders (VAEs) that models the output data as a Bernoulli distribution. This is typically used when the outputs of the decoder are probabilities.</p><p><strong>Fields</strong></p><ul><li><code>decoder::Flux.Chain</code>: The primary neural network used to process the latent   space and map it to the output (or reconstructed) space.</li></ul><p><strong>Description</strong></p><p><code>BernoulliDecoder</code> represents a VAE decoder that models the output data as a Bernoulli distribution. It&#39;s commonly used when the outputs of the decoder are probabilities, such as in a binary classification task or when modeling binary data. Unlike a Gaussian decoder, there&#39;s no need for separate paths or operations on the mean or log standard deviation.</p><p><strong>Note</strong></p><p>Ensure the last layer of the decoder outputs a value between 0 and 1, as this is required for a Bernoulli distribution.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.BernoulliDecoder-Tuple{AbstractArray}" href="#AutoEncode.BernoulliDecoder-Tuple{AbstractArray}"><code>AutoEncode.BernoulliDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (decoder::BernoulliDecoder)(z::AbstractArray)</code></pre><p>Maps the given latent representation <code>z</code> through the <code>BernoulliDecoder</code> network to reconstruct the original input.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: The latent space representation to be decoded.   This can be a vector or a matrix, where each column represents a separate   sample from the latent space of a VAE.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(p=p,)</code> where <code>p</code> is an array representing the output of the   decoder, which should resemble the original input to the VAE (post encoding   and sampling from the latent space).</li></ul><p><strong>Description</strong></p><p>This function processes the latent space representation <code>z</code> using the neural network defined in the <code>BernoulliDecoder</code> struct. The aim is to decode or reconstruct the original input from this representation.</p><p><strong>Note</strong></p><p>Ensure that the latent space representation z matches the expected input dimensionality for the BernoulliDecoder.</p></div></section></article><h3 id="CategoricalDecoder"><a class="docs-heading-anchor" href="#CategoricalDecoder"><code>CategoricalDecoder</code></a><a id="CategoricalDecoder-1"></a><a class="docs-heading-anchor-permalink" href="#CategoricalDecoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.CategoricalDecoder" href="#AutoEncode.CategoricalDecoder"><code>AutoEncode.CategoricalDecoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CategoricalDecoder &lt;: AbstractVariationalDecoder</code></pre><p>A decoder structure for variational autoencoders (VAEs) that models the output data as a categorical distribution. This is typically used when the outputs of the decoder are categorical variables encoded as one-hot vectors.</p><p><strong>Fields</strong></p><ul><li><code>decoder::Flux.Chain</code>: The primary neural network used to process the latent space and map it to the output (or reconstructed) space.</li></ul><p><strong>Description</strong></p><p><code>CategoricalDecoder</code> represents a VAE decoder that models the output data as a categorical distribution. It&#39;s commonly used when the outputs of the decoder are categorical variables, such as in a multi-class one-hot encoded vectors. Unlike a Gaussian decoder, there&#39;s no need for separate paths or operations on the mean or log standard deviation.</p><p><strong>Note</strong></p><p>Ensure the last layer of the decoder outputs a probability distribution over the categories, as this is required for a categorical distribution. This can be done using a softmax activation function, for example.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.CategoricalDecoder-Tuple{AbstractArray}" href="#AutoEncode.CategoricalDecoder-Tuple{AbstractArray}"><code>AutoEncode.CategoricalDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(decoder::CategoricalDecoder)(z::AbstractArray)</code></pre><p>Maps the given latent representation <code>z</code> through the <code>CategoricalDecoder</code> network to reconstruct the original input.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: The latent space representation to be decoded.  This can be a vector or a matrix, where each column represents a separate sample from the latent space of a VAE.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(p=p,)</code> where <code>p</code> is an array representing the output of the decoder, which should resemble the original input to the VAE (post encoding and sampling from the latent space).</li></ul><p><strong>Description</strong></p><p>This function processes the latent space representation <code>z</code> using the neural network defined in the <code>CategoricalDecoder</code> struct. The aim is to decode or reconstruct the original input from this representation.</p><p><strong>Note</strong></p><p>Ensure that the latent space representation z matches the expected input dimensionality for the CategoricalDecoder.</p></div></section></article><h3 id="JointDecoder"><a class="docs-heading-anchor" href="#JointDecoder"><code>JointDecoder</code></a><a id="JointDecoder-1"></a><a class="docs-heading-anchor-permalink" href="#JointDecoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointDecoder" href="#AutoEncode.JointDecoder"><code>AutoEncode.JointDecoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointDecoder &lt;: AbstractGaussianLinearDecoder</code></pre><p>An extended decoder structure for VAEs that incorporates separate layers for mapping from the latent space to both its mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>).</p><p><strong>Fields</strong></p><ul><li><code>decoder::Flux.Chain</code>: The primary neural network used to process the latent space before determining its mean and log standard deviation.</li><li><code>Âµ::Flux.Dense</code>: A dense layer that maps from the output of the <code>decoder</code> to the mean of the latent space.</li><li><code>Ïƒ::Flux.Dense</code>: A dense layer that maps from the output of the <code>decoder</code> to the standard deviation of the latent space.</li></ul><p><strong>Description</strong></p><p><code>JointDecoder</code> is tailored for VAE architectures where the same decoder network is used initially, and then splits into two separate paths for determining both the mean and standard deviation of the latent space.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointDecoder-Tuple{AbstractArray}" href="#AutoEncode.JointDecoder-Tuple{AbstractArray}"><code>AutoEncode.JointDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (decoder::JointDecoder)(z::AbstractArray)</code></pre><p>Maps the given latent representation <code>z</code> through the <code>JointDecoder</code> network to produce both the mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: The latent space representation to be decoded. If array, the last dimension contains each of the latent space representations to be decoded.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(Âµ=Âµ, Ïƒ=Ïƒ,)</code> where:<ul><li><code>Âµ::AbstractArray</code>: The mean representation obtained from the decoder.</li><li><code>Ïƒ::AbstractArray</code>: The standard deviation representation obtained from the decoder.</li></ul></li></ul><p><strong>Description</strong></p><p>This function processes the latent space representation <code>z</code> using the primary neural network of the <code>JointDecoder</code> struct. It then separately maps the output of this network to the mean and standard deviation using the <code>Âµ</code> and <code>Ïƒ</code> dense layers, respectively.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">decoder = JointDecoder(...)
z = ... # some latent space representation
output = decoder(z)</code></pre><p><strong>Note</strong></p><p>Ensure that the latent space representation z matches the expected input dimensionality for the JointDecoder.</p></div></section></article><h3 id="JointLogDecoder"><a class="docs-heading-anchor" href="#JointLogDecoder"><code>JointLogDecoder</code></a><a id="JointLogDecoder-1"></a><a class="docs-heading-anchor-permalink" href="#JointLogDecoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogDecoder" href="#AutoEncode.JointLogDecoder"><code>AutoEncode.JointLogDecoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointLogDecoder &lt;: AbstractGaussianLogDecoder</code></pre><p>An extended decoder structure for VAEs that incorporates separate layers for mapping from the latent space to both its mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Fields</strong></p><ul><li><code>decoder::Flux.Chain</code>: The primary neural network used to process the latent space before determining its mean and log standard deviation.</li><li><code>Âµ::Flux.Dense</code>: A dense layer that maps from the output of the <code>decoder</code> to the mean of the latent space.</li><li><code>logÏƒ::Flux.Dense</code>: A dense layer that maps from the output of the <code>decoder</code> to the log standard deviation of the latent space.</li></ul><p><strong>Description</strong></p><p><code>JointLogDecoder</code> is tailored for VAE architectures where the same decoder network is used initially, and then splits into two separate paths for determining both the mean and log standard deviation of the latent space.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogDecoder-Tuple{AbstractArray}" href="#AutoEncode.JointLogDecoder-Tuple{AbstractArray}"><code>AutoEncode.JointLogDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (decoder::JointLogDecoder)(z::AbstractArray)</code></pre><p>Maps the given latent representation <code>z</code> through the <code>JointLogDecoder</code> network to produce both the mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: The latent space representation to be decoded. If array, the last dimension contains each of the latent space representations.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(Âµ=Âµ, logÏƒ=logÏƒ,)</code> where:<ul><li><code>Âµ::Array</code>: The mean representation obtained from the decoder.</li><li><code>logÏƒ::Array</code>: The log standard deviation representation obtained from the decoder.</li></ul></li></ul><p><strong>Description</strong></p><p>This function processes the latent space representation <code>z</code> using the primary neural network of the <code>JointLogDecoder</code> struct. It then separately maps the output of this network to the mean and log standard deviation using the <code>Âµ</code> and <code>logÏƒ</code> dense layers, respectively.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">decoder = JointLogDecoder(...)
z = ... # some latent space representation
output = decoder(z)</code></pre><p><strong>Note</strong></p><p>Ensure that the latent space representation z matches the expected input dimensionality for the JointLogDecoder.</p></div></section></article><h3 id="SplitDecoder"><a class="docs-heading-anchor" href="#SplitDecoder"><code>SplitDecoder</code></a><a id="SplitDecoder-1"></a><a class="docs-heading-anchor-permalink" href="#SplitDecoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SplitDecoder" href="#AutoEncode.SplitDecoder"><code>AutoEncode.SplitDecoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SplitDecoder &lt;: AbstractGaussianLinearDecoder</code></pre><p>A specialized decoder structure for VAEs that uses distinct neural networks for determining the mean (<code>Âµ</code>) and standard deviation (<code>logÏƒ</code>) of the latent space.</p><p><strong>Fields</strong></p><ul><li><code>decoder_Âµ::Flux.Chain</code>: A neural network dedicated to processing the latent space and mapping it to its mean.</li><li><code>decoder_Ïƒ::Flux.Chain</code>: A neural network dedicated to processing the latent space and mapping it to its standard deviation.</li></ul><p><strong>Description</strong></p><p><code>SplitDecoder</code> is designed for VAE architectures where separate decoder networks are preferred for computing the mean and log standard deviation, ensuring that each has its own distinct set of parameters and transformation logic.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SplitDecoder-Tuple{AbstractArray}" href="#AutoEncode.SplitDecoder-Tuple{AbstractArray}"><code>AutoEncode.SplitDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (decoder::SplitDecoder)(z::AbstractArray)</code></pre><p>Maps the given latent representation <code>z</code> through the separate networks of the <code>SplitDecoder</code> to produce both the mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: The latent space representation to be decoded. If array, the last dimension contains each of the latent space representations to be decoded.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(Âµ=Âµ, Ïƒ=Ïƒ,)</code> where:<ul><li><code>Âµ::AbstractArray</code>: The mean representation obtained using the dedicated <code>decoder_Âµ</code> network.</li><li><code>Ïƒ::AbstractArray</code>: The standard deviation representation obtained using the dedicated <code>decoder_Ïƒ</code> network.</li></ul></li></ul><p><strong>Description</strong></p><p>This function processes the latent space representation <code>z</code> through two distinct neural networks within the <code>SplitDecoder</code> struct. The <code>decoder_Âµ</code> network is used to produce the mean representation, while the <code>decoder_Ïƒ</code> network is utilized for the standard deviation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">decoder = SplitDecoder(...)
z = ... # some latent space representation
output = decoder(z)</code></pre><p><strong>Note</strong></p><p>Ensure that the latent space representation z matches the expected input dimensionality for both networks in the SplitDecoder.</p></div></section></article><h3 id="SplitLogDecoder"><a class="docs-heading-anchor" href="#SplitLogDecoder"><code>SplitLogDecoder</code></a><a id="SplitLogDecoder-1"></a><a class="docs-heading-anchor-permalink" href="#SplitLogDecoder" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SplitLogDecoder" href="#AutoEncode.SplitLogDecoder"><code>AutoEncode.SplitLogDecoder</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SplitLogDecoder &lt;: AbstractGaussianLogDecoder</code></pre><p>A specialized decoder structure for VAEs that uses distinct neural networks for determining the mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>) of the latent space.</p><p><strong>Fields</strong></p><ul><li><code>decoder_Âµ::Flux.Chain</code>: A neural network dedicated to processing the latent space and mapping it to its mean.</li><li><code>decoder_logÏƒ::Flux.Chain</code>: A neural network dedicated to processing the latent space and mapping it to its log standard deviation.</li></ul><p><strong>Description</strong></p><p><code>SplitLogDecoder</code> is designed for VAE architectures where separate decoder networks are preferred for computing the mean and log standard deviation, ensuring that each has its own distinct set of parameters and transformation logic.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SplitLogDecoder-Tuple{AbstractArray}" href="#AutoEncode.SplitLogDecoder-Tuple{AbstractArray}"><code>AutoEncode.SplitLogDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (decoder::SplitLogDecoder)(z::AbstractArray)</code></pre><p>Maps the given latent representation <code>z</code> through the separate networks of the <code>SplitLogDecoder</code> to produce both the mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractArray</code>: The latent space representation to be decoded. If array, the last dimension contains each of the latent space representations to be decoded.</li></ul><p><strong>Returns</strong></p><ul><li>A NamedTuple <code>(Âµ=Âµ, logÏƒ=logÏƒ,)</code> where:<ul><li><code>Âµ::AbstractArray</code>: The mean representation obtained using the dedicated <code>decoder_Âµ</code> network.</li><li><code>logÏƒ::AbstractArray</code>: The log standard deviation representation obtained using the dedicated <code>decoder_logÏƒ</code> network.</li></ul></li></ul><p><strong>Description</strong></p><p>This function processes the latent space representation <code>z</code> through two distinct neural networks within the <code>SplitLogDecoder</code> struct. The <code>decoder_Âµ</code> network is used to produce the mean representation, while the <code>decoder_logÏƒ</code> network is utilized for the log standard deviation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">decoder = SplitLogDecoder(...)
z = ... # some latent space representation
output = decoder(z))</code></pre><p><strong>Note</strong></p><p>Ensure that the latent space representation z matches the expected input dimensionality for both networks in the SplitLogDecoder.</p></div></section></article><h2 id="Default-initializations"><a class="docs-heading-anchor" href="#Default-initializations">Default initializations</a><a id="Default-initializations-1"></a><a class="docs-heading-anchor-permalink" href="#Default-initializations" title="Permalink"></a></h2><p>The package provides a set of functions to initialize encoder and decoder architectures. Although it gives the user less flexibility, it can be useful for quick prototyping.</p><h3 id="Encoder-initializations"><a class="docs-heading-anchor" href="#Encoder-initializations">Encoder initializations</a><a id="Encoder-initializations-1"></a><a class="docs-heading-anchor-permalink" href="#Encoder-initializations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.Encoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.Encoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.Encoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Encoder(n_input, n_latent, latent_activation, encoder_neurons, 
        encoder_activation; init=Flux.glorot_uniform)</code></pre><p>Construct and initialize an <code>Encoder</code> struct that defines an encoder network for a deterministic autoencoder.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: The dimensionality of the input data.</li><li><code>n_latent::Int</code>: The dimensionality of the latent space.</li><li><code>encoder_neurons::Vector{&lt;:Int}</code>: A vector specifying the number of neurons in each layer of the encoder network.</li><li><code>encoder_activation::Vector{&lt;:Function}</code>: Activation functions corresponding to each layer in the <code>encoder_neurons</code>.</li><li><code>latent_activation::Function</code>: Activation function for the latent space layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: The initialization function used for the neural network weights.</li></ul><p><strong>Returns</strong></p><ul><li>An <code>Encoder</code> struct initialized based on the provided arguments.</li></ul><p><strong>Examples</strong></p><p><code>julia encoder = Encoder(784, 20, tanh, [400], [relu])</code>`</p><p><strong>Notes</strong></p><p>The length of encoder<em>neurons should match the length of encoder</em>activation, ensuring that each layer in the encoder has a corresponding activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogEncoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.JointLogEncoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.JointLogEncoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointLogEncoder(n_input, n_latent, encoder_neurons, encoder_activation, 
             latent_activation; init=Flux.glorot_uniform)</code></pre><p>Construct and initialize a <code>JointLogEncoder</code> struct that defines an encoder network for a variational autoencoder.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: The dimensionality of the input data.</li><li><code>n_latent::Int</code>: The dimensionality of the latent space.</li><li><code>encoder_neurons::Vector{&lt;:Int}</code>: A vector specifying the number of neurons in each layer of the encoder network.</li><li><code>encoder_activation::Vector{&lt;:Function}</code>: Activation functions corresponding to each layer in the <code>encoder_neurons</code>.</li><li><code>latent_activation::Function</code>: Activation function for the latent space layers (both Âµ and logÏƒ).</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: The initialization function used for the neural network weights.</li></ul><p><strong>Returns</strong></p><ul><li>A <code>JointLogEncoder</code> struct initialized based on the provided arguments.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">encoder = JointLogEncoder(784, 20, [400], [relu], tanh)</code></pre><p><strong>Notes</strong></p><p>The length of encoder<em>neurons should match the length of encoder</em>activation, ensuring that each layer in the encoder has a corresponding activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogEncoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}" href="#AutoEncode.JointLogEncoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}"><code>AutoEncode.JointLogEncoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointLogEncoder(n_input, n_latent, encoder_neurons, encoder_activation, 
             latent_activation; init=Flux.glorot_uniform)</code></pre><p>Construct and initialize a <code>JointLogEncoder</code> struct that defines an encoder network for a variational autoencoder.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: The dimensionality of the input data.</li><li><code>n_latent::Int</code>: The dimensionality of the latent space.</li><li><code>encoder_neurons::Vector{&lt;:Int}</code>: A vector specifying the number of neurons in each layer of the encoder network.</li><li><code>encoder_activation::Vector{&lt;:Function}</code>: Activation functions corresponding to each layer in the <code>encoder_neurons</code>.</li><li><code>latent_activation::Vector{&lt;:Function}</code>: Activation functions for the latent space layers (both Âµ and logÏƒ).</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: The initialization function used for the neural network weights.</li></ul><p><strong>Returns</strong></p><ul><li>A <code>JointLogEncoder</code> struct initialized based on the provided arguments.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">encoder = JointLogEncoder(784, 20, [400], [relu], tanh)</code></pre><p><strong>Notes</strong></p><p>The length of encoder<em>neurons should match the length of encoder</em>activation, ensuring that each layer in the encoder has a corresponding activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointEncoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}" href="#AutoEncode.JointEncoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}"><code>AutoEncode.JointEncoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointEncoder(n_input, n_latent, encoder_neurons, encoder_activation, 
             latent_activation; init=Flux.glorot_uniform)</code></pre><p>Construct and initialize a <code>JointLogEncoder</code> struct that defines an encoder network for a variational autoencoder.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: The dimensionality of the input data.</li><li><code>n_latent::Int</code>: The dimensionality of the latent space.</li><li><code>encoder_neurons::Vector{&lt;:Int}</code>: A vector specifying the number of neurons in each layer of the encoder network.</li><li><code>encoder_activation::Vector{&lt;:Function}</code>: Activation functions corresponding to each layer in the <code>encoder_neurons</code>.</li><li><code>latent_activation::Vector{&lt;:Function}</code>: Activation function for the latent space layers. This vector must contain the activation for both Âµ and logÏƒ.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: The initialization function used for the neural network weights.</li></ul><p><strong>Returns</strong></p><ul><li>A <code>JointEncoder</code> struct initialized based on the provided arguments.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">encoder = JointEncoder(784, 20, [400], [relu], [tanh, softplus])</code></pre><p><strong>Notes</strong></p><p>The length of encoder<em>neurons should match the length of encoder</em>activation, ensuring that each layer in the encoder has a corresponding activation function.</p></div></section></article><h3 id="Decoder-initializations"><a class="docs-heading-anchor" href="#Decoder-initializations">Decoder initializations</a><a id="Decoder-initializations-1"></a><a class="docs-heading-anchor-permalink" href="#Decoder-initializations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.Decoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.Decoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.Decoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Decoder(n_input, n_latent, decoder_neurons, decoder_activation, 
        output_activation; init=Flux.glorot_uniform)</code></pre><p>Construct and initialize a <code>Decoder</code> struct that defines a decoder network for a deterministic autoencoder.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: The dimensionality of the output data (which typically matches the input data dimensionality of the autoencoder).</li><li><code>n_latent::Int</code>: The dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: A vector specifying the number of neurons in each layer of the decoder network.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions corresponding to each layer in the <code>decoder_neurons</code>.</li><li><code>output_activation::Function</code>: Activation function for the final output layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: The initialization function used for the neural network weights.</li></ul><p><strong>Returns</strong></p><ul><li>A <code>Decoder</code> struct initialized based on the provided arguments.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">decoder = Decoder(784, 20, sigmoid, [400], [relu])</code></pre><p><strong>Notes</strong></p><p>The length of decoder<em>neurons should match the length of decoder</em>activation, ensuring that each layer in the decoder has a corresponding activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SimpleDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.SimpleDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.SimpleDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">SimpleDecoder(n_input, n_latent, decoder_neurons, decoder_activation, 
            output_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>SimpleDecoder</code> object designed for variational autoencoders (VAEs). This function sets up a straightforward decoder network that maps from a latent space to an output space.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the decoder network, not including the input latent layer and the final output layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each decoder layer, not including the final output layer.</li><li><code>output_activation::Function</code>: Activation function for the final output layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>SimpleDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>SimpleDecoder</code> object, setting up its decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space, goes through a sequence of middle layers if specified, and finally maps to the output space.</p><p>The function ensures that there are appropriate activation functions provided for each layer in the <code>decoder_neurons</code> and checks for potential mismatches in length.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_input = 28*28
n_latent = 64
decoder_neurons = [128, 256]
decoder_activation = [relu, relu]
output_activation = sigmoid
decoder = SimpleDecoder(
    n_input, n_latent, decoder_neurons, decoder_activation, output_activation
)</code></pre><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match, excluding the output layer.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.JointLogDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.JointLogDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointLogDecoder(n_input, n_latent, decoder_neurons, decoder_activation, 
            latent_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>JointLogDecoder</code> object for variational autoencoders (VAEs). This function sets up a decoder network that first processes the latent space and then maps it separately to both its mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the primary decoder network, not including the input latent layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each primary decoder layer.</li><li><code>output_activation::Function</code>: Activation function for the mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>) layers.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>JointLogDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>JointLogDecoder</code> object, setting up its primary decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space and goes through a sequence of middle layers if specified. After processing the latent space through the primary decoder, it then maps separately to both its mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_input = 28*28
n_latent = 64
decoder_neurons = [128, 256]
decoder_activation = [relu, relu]
output_activation = tanh
decoder = JointLogDecoder(
    n_input, n_latent, decoder_neurons, decoder_activation, output_activation
)</code></pre><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointLogDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}" href="#AutoEncode.JointLogDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}"><code>AutoEncode.JointLogDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointLogDecoder(n_input, n_latent, decoder_neurons, decoder_activation, 
            latent_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>JointLogDecoder</code> object for variational autoencoders (VAEs). This function sets up a decoder network that first processes the latent space and then maps it separately to both its mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the primary decoder network, not including the input latent layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each primary decoder layer.</li><li><code>output_activation::Vector{&lt;:Function}</code>: Activation functions for the mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>) layers.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>JointLogDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>JointLogDecoder</code> object, setting up its primary decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space and goes through a sequence of middle layers if specified. After processing the latent space through the primary decoder, it then maps separately to both its mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_input = 28*28
n_latent = 64
decoder_neurons = [128, 256]
decoder_activation = [relu, relu]
output_activation = [tanh, identity]
decoder = JointLogDecoder(
    n_input, n_latent, decoder_neurons, decoder_activation, latent_activation
)</code></pre><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.JointDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.JointDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointDecoder(n_input, n_latent, decoder_neurons, decoder_activation, 
            latent_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>JointLogDecoder</code> object for variational autoencoders (VAEs). This function sets up a decoder network that first processes the latent space and then maps it separately to both its mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the primary decoder network, not including the input latent layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each primary decoder layer.</li><li><code>output_activation::Function</code>: Activation function for the mean (<code>Âµ</code>) and log standard deviation (<code>logÏƒ</code>) layers.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>JointDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>JointDecoder</code> object, setting up its primary decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space and goes through a sequence of middle layers if specified. After processing the latent space through the primary decoder, it then maps separately to both its mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_input = 28*28
n_latent = 64
decoder_neurons = [128, 256]
decoder_activation = [relu, relu]
output_activation = tanh
decoder = JointDecoder(
    n_input, n_latent, decoder_neurons, decoder_activation, output_activation
)</code></pre><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.JointDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}" href="#AutoEncode.JointDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Function}}"><code>AutoEncode.JointDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">JointDecoder(n_input, n_latent, decoder_neurons, decoder_activation, 
            latent_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>JointDecoder</code> object for variational autoencoders (VAEs). This function sets up a decoder network that first processes the latent space and then maps it separately to both its mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>).</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the primary decoder network, not including the input latent layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each primary decoder layer.</li><li><code>output_activation::Function</code>: Activation function for the mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>) layers.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>JointDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>JointDecoder</code> object, setting up its primary decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space and goes through a sequence of middle layers if specified. After processing the latent space through the primary decoder, it then maps separately to both its mean (<code>Âµ</code>) and standard deviation (<code>Ïƒ</code>).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_input = 28*28
n_latent = 64
decoder_neurons = [128, 256]
decoder_activation = [relu, relu]
latent_activation = [tanh, softplus]
decoder = JointDecoder(
    n_input, n_latent, decoder_neurons, decoder_activation, latent_activation
)</code></pre><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SplitLogDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Int64}, Vector{&lt;:Function}}" href="#AutoEncode.SplitLogDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Int64}, Vector{&lt;:Function}}"><code>AutoEncode.SplitLogDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">SplitLogDecoder(n_input, n_latent, Âµ_neurons, Âµ_activation, logÏƒ_neurons, 
            logÏƒ_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>SplitLogDecoder</code> object for variational autoencoders (VAEs). This function sets up two distinct decoder networks, one dedicated for determining the mean (<code>Âµ</code>) and the other for the log standard deviation (<code>logÏƒ</code>) of the latent space.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>Âµ_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the <code>Âµ</code> decoder network, not including the input latent layer.</li><li><code>Âµ_activation::Vector{&lt;:Function}</code>: Activation functions for each <code>Âµ</code> decoder layer.</li><li><code>logÏƒ_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the <code>logÏƒ</code> decoder network, not including the input latent layer.</li><li><code>logÏƒ_activation::Vector{&lt;:Function}</code>: Activation functions for each <code>logÏƒ</code> decoder layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>SplitLogDecoder</code> object with two distinct networks initialized with the specified architectures and weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>SplitLogDecoder</code> object, setting up two separate decoder networks based on the provided specifications. The first network, dedicated to determining the mean (<code>Âµ</code>), and the second for the log standard deviation (<code>logÏƒ</code>), both begin with a dense layer mapping from the latent space and go through a sequence of middle layers if specified.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_latent = 64
Âµ_neurons = [128, 256]
Âµ_activation = [relu, relu]
logÏƒ_neurons = [128, 256]
logÏƒ_activation = [relu, relu]
decoder = SplitLogDecoder(
    n_latent, Âµ_neurons, Âµ_activation, logÏƒ_neurons, logÏƒ_activation
)</code></pre><p><strong>Notes</strong></p><ul><li>Ensure that the lengths of Âµ<em>neurons with Âµ</em>activation and logÏƒ<em>neurons with logÏƒ</em>activation match respectively.</li><li>If Âµ<em>neurons[end] or logÏƒ</em>neurons[end] do not match n_input, the function automatically changes this number to match the right dimensionality</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.SplitDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Int64}, Vector{&lt;:Function}}" href="#AutoEncode.SplitDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Vector{&lt;:Int64}, Vector{&lt;:Function}}"><code>AutoEncode.SplitDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">SplitDecoder(n_input, n_latent, Âµ_neurons, Âµ_activation, logÏƒ_neurons, 
            logÏƒ_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>SplitDecoder</code> object for variational autoencoders (VAEs). This function sets up two distinct decoder networks, one dedicated for determining the mean (<code>Âµ</code>) and the other for the standard deviation (<code>Ïƒ</code>) of the latent space.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>Âµ_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the <code>Âµ</code> decoder network, not including the input latent layer.</li><li><code>Âµ_activation::Vector{&lt;:Function}</code>: Activation functions for each <code>Âµ</code> decoder layer.</li><li><code>Ïƒ_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the <code>Ïƒ</code> decoder network, not including the input latent layer.</li><li><code>Ïƒ_activation::Vector{&lt;:Function}</code>: Activation functions for each <code>Ïƒ</code> decoder layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>SplitDecoder</code> object with two distinct networks initialized with the specified architectures and weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>SplitDecoder</code> object, setting up two separate decoder networks based on the provided specifications. The first network, dedicated to determining the mean (<code>Âµ</code>), and the second for the standard deviation (<code>Ïƒ</code>), both begin with a dense layer mapping from the latent space and go through a sequence of middle layers if specified.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_latent = 64
Âµ_neurons = [128, 256]
Âµ_activation = [relu, relu]
Ïƒ_neurons = [128, 256]
Ïƒ_activation = [relu, relu]
decoder = SplitDecoder(
    n_latent, Âµ_neurons, Âµ_activation, Ïƒ_neurons, Ïƒ_activation
)</code></pre><p><strong>Notes</strong></p><ul><li>Ensure that the lengths of Âµ<em>neurons with Âµ</em>activation and Ïƒ<em>neurons with Ïƒ</em>activation match respectively.</li><li>If Âµ<em>neurons[end] or Ïƒ</em>neurons[end] do not match n_input, the function automatically changes this number to match the right dimensionality</li><li>Ensure that Ïƒ_neurons[end] maps to a <strong>positive</strong> value. Activation functions such as <code>softplus</code> are needed to guarantee the positivity of the standard deviation.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.BernoulliDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.BernoulliDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.BernoulliDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    BernoulliDecoder(n_input, n_latent, decoder_neurons, decoder_activation, 
                            output_activation; init=Flux.glorot_uniform)</code></pre><p>Constructs and initializes a <code>BernoulliDecoder</code> object designed for variational autoencoders (VAEs). This function sets up a decoder network that maps from a latent space to an output space.</p><p><strong>Arguments</strong></p><ul><li><code>n_input::Int</code>: Dimensionality of the output data (or the data to be   reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the decoder   network, not including the input latent layer and the final output layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each   decoder layer, not including the final output layer.</li><li><code>output_activation::Function</code>: Activation function for the final output layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network   parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>BernoulliDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>BernoulliDecoder</code> object, setting up its decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space, goes through a sequence of middle layers if specified, and finally maps to the output space.</p><p>The function ensures that there are appropriate activation functions provided for each layer in the <code>decoder_neurons</code> and checks for potential mismatches in length.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">n_input = 28*28
n_latent = 64
decoder_neurons = [128, 256]
decoder_activation = [relu, relu]
output_activation = sigmoid
decoder = BernoulliDecoder(
    n_input, 
    n_latent, 
    decoder_neurons, 
    decoder_activation, 
    output_activation
)</code></pre><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match, excluding the output layer. Also, the output activation function should return values between 0 and 1, as the decoder models the output data as a Bernoulli distribution. </p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.CategoricalDecoder-Tuple{AbstractVector{&lt;:Int64}, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.CategoricalDecoder-Tuple{AbstractVector{&lt;:Int64}, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.CategoricalDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    CategoricalDecoder(
        size_input, n_latent, decoder_neurons, decoder_activation, 
        output_activation; init=Flux.glorot_uniform
    )</code></pre><p>Constructs and initializes a <code>CategoricalDecoder</code> object designed for variational autoencoders (VAEs). This function sets up a decoder network that maps from a latent space to an output space.</p><p><strong>Arguments</strong></p><ul><li><code>size_input::AbstractVector{&lt;:Int}</code>: Dimensionality of the output data (or the data to be reconstructed) in the form of a vector where each element represents the size of a dimension.</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the decoder network, not including the input latent layer and the final output layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each decoder layer, not including the final output layer.</li><li><code>output_activation::Function</code>: Activation function for the final output layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>CategoricalDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>CategoricalDecoder</code> object, setting up its decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space, goes through a sequence of middle layers if specified, and finally maps to the output space.</p><p>The function ensures that there are appropriate activation functions provided for each layer in the <code>decoder_neurons</code> and checks for potential mismatches in length.</p><p>The output layer uses the identity function as its activation function, and the output is reshaped to match the dimensions specified in <code>size_input</code>. The <code>output_activation</code> function is then applied over the first dimension of the reshaped output.</p><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match, excluding the output layer. Also, the output activation function should return values that can be interpreted as probabilities, as the decoder models the output data as a categorical distribution. </p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.CategoricalDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}" href="#AutoEncode.CategoricalDecoder-Tuple{Int64, Int64, Vector{&lt;:Int64}, Vector{&lt;:Function}, Function}"><code>AutoEncode.CategoricalDecoder</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">CategoricalDecoder(
    n_input, n_latent, decoder_neurons, decoder_activation,
    output_activation; init=Flux.glorot_uniform
)</code></pre><p>Constructs and initializes a <code>CategoricalDecoder</code> object designed for variational autoencoders (VAEs). This function sets up a decoder network that maps from a latent space to an output space.</p><p><strong>Arguments</strong></p><ul><li><code>size_input::AbstractVector{&lt;:Int}</code>: Dimensionality of the output data (or the data to be reconstructed).</li><li><code>n_latent::Int</code>: Dimensionality of the latent space.</li><li><code>decoder_neurons::Vector{&lt;:Int}</code>: Vector of layer sizes for the decoder network, not including the input latent layer and the final output layer.</li><li><code>decoder_activation::Vector{&lt;:Function}</code>: Activation functions for each decoder layer, not including the final output layer.</li><li><code>output_activation::Function</code>: Activation function for the final output layer.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>init::Function=Flux.glorot_uniform</code>: Initialization function for the network parameters.</li></ul><p><strong>Returns</strong></p><p>A <code>CategoricalDecoder</code> object with the specified architecture and initialized weights.</p><p><strong>Description</strong></p><p>This function constructs a <code>CategoricalDecoder</code> object, setting up its decoder network based on the provided specifications. The architecture begins with a dense layer mapping from the latent space, goes through a sequence of middle layers if specified, and finally maps to the output space.</p><p>The function ensures that there are appropriate activation functions provided for each layer in the <code>decoder_neurons</code> and checks for potential mismatches in length.</p><p><strong>Note</strong></p><p>Ensure that the lengths of decoder<em>neurons and decoder</em>activation match, excluding the output layer. Also, the output activation function should return values that can be interpreted as probabilities, as the decoder models the output data as a categorical distribution. </p></div></section></article><h2 id="Probabilistic-functions"><a class="docs-heading-anchor" href="#Probabilistic-functions">Probabilistic functions</a><a id="Probabilistic-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-functions" title="Permalink"></a></h2><p>Given the probability-centered design of <code>AutoEncode.jl</code>, each variational encoder and decoder has an associated probabilistic function used when computing the evidence lower bound (ELBO). The following functions are available:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.encoder_logposterior" href="#AutoEncode.encoder_logposterior"><code>AutoEncode.encoder_logposterior</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">encoder_logposterior(
    z::AbstractVector,
    encoder::AbstractGaussianLogEncoder,
    encoder_output::NamedTuple
)</code></pre><p>Computes the log-posterior of the latent variable <code>z</code> given the encoder output under a Gaussian distribution with mean and standard deviation given by the encoder.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractVector</code>: The latent variable for which the log-posterior is to be computed.</li><li><code>encoder::AbstractGaussianLogEncoder</code>: The encoder of the VAE, which is not used in the computation of the log-posterior. This argument is only used to know which method to call.</li><li><code>encoder_output::NamedTuple</code>: The output of the encoder, which includes the mean and log standard deviation of the Gaussian distribution.</li></ul><p><strong>Returns</strong></p><ul><li><code>logposterior::T</code>: The computed log-posterior of the latent variable <code>z</code> given the encoder output.</li></ul><p><strong>Description</strong></p><p>The function computes the log-posterior of the latent variable <code>z</code> given the encoder output under a Gaussian distribution. The mean and log standard deviation of the Gaussian distribution are extracted from the <code>encoder_output</code>. The standard deviation is then computed by exponentiating the log standard deviation. The log-posterior is computed using the formula for the log-posterior of a Gaussian distribution.</p><p><strong>Note</strong></p><p>Ensure the dimensions of <code>z</code> match the expected input dimensionality of the <code>encoder</code>.</p></div></section><section><div><pre><code class="language-julia hljs">encoder_logposterior(
    z::AbstractMatrix,
    encoder::AbstractGaussianLogEncoder,
    encoder_output::NamedTuple
)</code></pre><p>Computes the log-posterior of the latent variable <code>z</code> given the encoder output under a Gaussian distribution with mean and standard deviation given by the encoder.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractMatrix</code>: The latent variable for which the log-posterior is to be computed. Each column of <code>z</code> represents a different data point.</li><li><code>encoder::AbstractGaussianLogEncoder</code>: The encoder of the VAE, which is not used in the computation of the log-posterior. This argument is only used to know which method to call.</li><li><code>encoder_output::NamedTuple</code>: The output of the encoder, which includes the mean and log standard deviation of the Gaussian distribution.</li></ul><p><strong>Returns</strong></p><ul><li><code>logposterior::Vector</code>: The computed log-posterior of the latent variable <code>z</code> given the encoder output. Each element of the vector corresponds to a different data point.</li></ul><p><strong>Description</strong></p><p>The function computes the log-posterior of the latent variable <code>z</code> given the encoder output under a Gaussian distribution. The mean and log standard deviation of the Gaussian distribution are extracted from the <code>encoder_output</code>. The standard deviation is then computed by exponentiating the log standard deviation. The log-posterior is computed using the formula for the log-posterior of a Gaussian distribution.</p><p><strong>Note</strong></p><p>Ensure the dimensions of <code>z</code> match the expected input dimensionality of the <code>encoder</code>.</p></div></section><section><div><pre><code class="language-julia hljs">encoder_logposterior(
    z::AbstractVector,
    encoder::AbstractGaussianLogEncoder,
    encoder_output::NamedTuple,
    index::Int
)</code></pre><p>Computes the log-posterior of the latent variable <code>z</code> for a single data point specified by <code>index</code> given the encoder output under a Gaussian distribution with mean and standard deviation given by the encoder.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractVector</code>: The latent variable for which the log-posterior is to be computed. </li><li><code>encoder::AbstractGaussianLogEncoder</code>: The encoder of the VAE, which is not used in the computation of the log-posterior. This argument is only used to know which method to call.</li><li><code>encoder_output::NamedTuple</code>: The output of the encoder, which includes the mean and log standard deviation of the Gaussian distribution for multiple data points.</li><li><code>index::Int</code>: The index of the data point for which the log-posterior is to be computed.</li></ul><p><strong>Returns</strong></p><ul><li><code>logposterior::Float32</code>: The computed log-posterior of the latent variable <code>z</code>   for the specified data point given the encoder output.</li></ul><p><strong>Description</strong></p><p>The function computes the log-posterior of the latent variable <code>z</code> for a single data point specified by <code>index</code> given the encoder output under a Gaussian distribution. The mean and log standard deviation of the Gaussian distribution are extracted from the <code>encoder_output</code> for the specified data point. The standard deviation is then computed by exponentiating the log standard deviation. The log-posterior is computed using the formula for the log-posterior of a Gaussian distribution.</p><p><strong>Note</strong></p><p>Ensure the dimensions of <code>z</code> match the expected input dimensionality of the <code>encoder</code>. Also, ensure that <code>index</code> is a valid index for the data points in <code>encoder_output</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.encoder_kl" href="#AutoEncode.encoder_kl"><code>AutoEncode.encoder_kl</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">encoder_kl(
    encoder::AbstractGaussianLogEncoder,
    encoder_output::NamedTuple
)</code></pre><p>Calculate the Kullback-Leibler (KL) divergence between the approximate posterior distribution and the prior distribution in a variational autoencoder with a Gaussian encoder.</p><p>The KL divergence for a Gaussian encoder with mean <code>encoder_Âµ</code> and log standard deviation <code>encoder_logÏƒ</code> is computed against a standard Gaussian prior.</p><p><strong>Arguments</strong></p><ul><li><code>encoder::AbstractGaussianLogEncoder</code>: Encoder network. This argument is not used in the computation of the KL divergence, but is included to allow for multiple encoder types to be used with the same function.</li><li><code>encoder_output::NamedTuple</code>: <code>NamedTuple</code> containing all the encoder outputs. It should have fields <code>Î¼</code> and <code>logÏƒ</code> representing the mean and log standard deviation of the encoder&#39;s output.</li></ul><p><strong>Returns</strong></p><ul><li><code>kl_div::Union{Number, Vector}</code>: The KL divergence for the entire batch of data points. If <code>encoder_Âµ</code> is a vector, <code>kl_div</code> is a scalar. If <code>encoder_Âµ</code> is a matrix, <code>kl_div</code> is a vector where each element corresponds to the KL divergence for a batch of data points.</li></ul><p><strong>Note</strong></p><ul><li>It is assumed that the mapping from data space to latent parameters (<code>encoder_Âµ</code> and <code>encoder_logÏƒ</code>) has been performed prior to calling this function. The <code>encoder</code> argument is provided to indicate the type of decoder network used, but it is not used within the function itself.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncode.spherical_logprior" href="#AutoEncode.spherical_logprior"><code>AutoEncode.spherical_logprior</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">spherical_logprior(z::AbstractVector, Ïƒ::Real=1.0f0)</code></pre><p>Computes the log-prior of the latent variable <code>z</code> under a spherical Gaussian distribution with zero mean and standard deviation <code>Ïƒ</code>.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractVector</code>: The latent variable for which the log-prior is to be computed.</li><li><code>Ïƒ::T=1.0f0</code>: The standard deviation of the spherical Gaussian distribution. Defaults to <code>1.0f0</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>logprior::T</code>: The computed log-prior of the latent variable <code>z</code>.</li></ul><p><strong>Description</strong></p><p>The function computes the log-prior of the latent variable <code>z</code> under a spherical Gaussian distribution with zero mean and standard deviation <code>Ïƒ</code>. The log-prior is computed using the formula for the log-prior of a Gaussian distribution.</p><p><strong>Note</strong></p><p>Ensure the dimension of <code>z</code> matches the expected dimensionality of the latent space.</p></div></section><section><div><pre><code class="language-julia hljs">spherical_logprior(z::AbstractMatrix, Ïƒ::Real=1.0f0)</code></pre><p>Computes the log-prior of the latent variable <code>z</code> under a spherical Gaussian distribution with zero mean and standard deviation <code>Ïƒ</code>.</p><p><strong>Arguments</strong></p><ul><li><code>z::AbstractMatrix</code>: The latent variable for which the log-prior is to be computed. Each column of <code>z</code> represents a different latent variable.</li><li><code>Ïƒ::Real=1.0f0</code>: The standard deviation of the spherical Gaussian distribution. Defaults to <code>1.0f0</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>logprior::T</code>: The computed log-prior(s) of the latent variable <code>z</code>.</li></ul><p><strong>Description</strong></p><p>The function computes the log-prior of the latent variable <code>z</code> under a spherical Gaussian distribution with zero mean and standard deviation <code>Ïƒ</code>. The log-prior is computed using the formula for the log-prior of a Gaussian distribution.</p><p><strong>Note</strong></p><p>Ensure the dimension of <code>z</code> matches the expected dimensionality of the latent space.</p></div></section></article><h2 id="Defining-custom-encoder-and-decoder-types"><a class="docs-heading-anchor" href="#Defining-custom-encoder-and-decoder-types">Defining custom encoder and decoder types</a><a id="Defining-custom-encoder-and-decoder-types-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-custom-encoder-and-decoder-types" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We will omit all docstrings in the following examples for brevity. However, every struct and function in <code>AutoEncode.jl</code> is well-documented.</p></div></div><p>Let us imagine your particular task requires a custom encoder or decoder type. For example, let&#39;s imagine that for a particular application, you need a decoder whose output distribution is Poisson. In other words, the assumption is that each dimension in the input <span>$x_i$</span> is a sample from a Poisson distribution with mean <span>$\lambda_i$</span>. Thus, on the decoder side, what the decoder return is a vector of these <span>$\lambda$</span> paraeters. We thus need to define a custom decoder type.</p><pre><code class="language-julia hljs">struct PoissonDecoder &lt;: AbstractVariationalDecoder
    decoder::Flux.Chain
end # struct</code></pre><p>With this struct defined, we need to define the forward-pass function for our custom <code>PoissonDecoder</code>. All decoders in <code>AutoEncode.jl</code> return a <code>NamedTuple</code> with the corresponding parameters of the distribution that defines them. In this case, the Poisson distribution is defined by a single parameter <span>$\lambda$</span>. Thus, we have a forward-pass of the form</p><pre><code class="language-julia hljs">function (decoder::PoissonDecoder)(z::AbstractArray)
    # Run input to decoder network
    return (Î»=decoder.decoder(z),)
end # function</code></pre><p>Next, we need to define the probabilistic function associated with this decoder. We know that the probability of observing <span>$x_i$</span> given <span>$\lambda_i$</span> is given by</p><p class="math-container">\[P(x_i | \lambda_i) = \frac{\lambda_i^{x_i} e^{-\lambda_i}}{x_i!}.
\tag{1}\]</p><p>If each <span>$x_i$</span> is independent, then the probability of observing the entire input <span>$x$</span> given the entire output <span>$\lambda$</span> is given by the product of the individual probabilities, i.e.</p><p class="math-container">\[P(x | \lambda) = \prod_i P(x_i | \lambda_i).
\tag{2}\]</p><p>The log-likehood of the data given the output of the decoder is then given by</p><p class="math-container">\[\mathcal{L}(x, \lambda) = \log P(x | \lambda) = \sum_i \log P(x_i | \lambda_i),
\tag{3}\]</p><p>which, by using the properties of the logarithm, can be written as</p><p class="math-container">\[\mathcal{L}(x, \lambda) = \sum_i x_i \log \lambda_i - \lambda_i - \log(x_i!).
\tag{4}\]</p><p>We can then define the probabilistic function associated with the <code>PoissonDecoder</code> as</p><pre><code class="language-julia hljs">function decoder_loglikelihood(
        x::AbstractArray,
        z::AbstractVector,
        decoder::PoissonDecoder,
        decoder_output::NamedTuple;
)
        # Extract the lambda parameter of the Poisson distribution
        Î» = decoder_output.Î»

        # Compute log-likelihood
        loglikelihood = sum(x .* log.(Î») - Î» - loggamma.(x .+ 1))

        return loglikelihood
end # function</code></pre><p>where we use the <code>loggamma</code> function from <code>SpecialFunctions.jl</code> to compute the log of the factorial of <code>x_i</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>We only defined the <code>decoder_loglikelihood</code> method for <code>z::AbstractVector</code>. One should also include a method for <code>z::AbstractMatrix</code> used when performing batch training.</p></div></div><p>With these two functions defined, our <code>PoissonDecoder</code> is ready to be used with any of the different VAE flavors included in <code>AutoEncode.jl</code>!</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">Â« Home</a><a class="docs-footer-nextpage" href="../ae/">Deterministic Autoencoders Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Monday 29 April 2024 20:23">Monday 29 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
